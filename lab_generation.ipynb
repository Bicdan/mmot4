{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Практическое задание 4\n",
    "# Генерация bash команды по текстовому запросу\n",
    "## курс \"Математические методы анализа текстов\"\n",
    "### ФИО: Бикметов Данил Наильевич"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи\n",
    "\n",
    "В этом задании вы построите систему, выдающую пользователю последовательность утилит командной строки linux (с нужными флагами) по его текстовому запросу. Вам дан набор пар текстовый запрос - команда на выходе. \n",
    "\n",
    "Решение этого задания будет построено на encoder-decoder архитектуре и модели transformer.\n",
    "\n",
    "\n",
    "### Библиотеки\n",
    "\n",
    "Для этого задания вам понадобятся следующие библиотеки:\n",
    "* pytorch\n",
    "* transformers\n",
    "* sentencepiece (bpe токенизация)\n",
    "* clai utils (скачать с гитхаба отсюда https://github.com/IBM/clai/tree/nlc2cmd/utils) \n",
    "\n",
    "\n",
    "### Данные\n",
    "\n",
    "В качестве обучающей выборке используются данные, сгенерированные автоматически по запросам с сайта stack overflow. В качестве тестовых данных используются пары запросов, размеченные асессорами.\n",
    "\n",
    "Данные можно скачать по ссылке: https://drive.google.com/file/d/1n457AAgrMwd5VbT6mGZ_rws3g2wwdEfX/view?usp=sharing\n",
    "\n",
    "### Метрика качества\n",
    "\n",
    "Ваш алгоритм должен выдавать пять вариантов ответа для каждого запроса. \n",
    "Для упрощения задачи метрика качества будет учитывать утилиты и флаги ответа, но не учитывать подставленные значения. Пусть $\\{ u_1, \\ldots, u_T \\}$, $\\{ f_1, \\ldots, f_T \\}$ --- список утилит и множества их флагов ответа алгоритма, $\\{v_1, \\ldots, v_T \\}$, $\\{ \\phi_1, \\ldots, \\phi_T \\}$ --- список утилит и множества их флагов эталонного ответа. Если ответы отличаются по длине, они дополняются `None` утилитой. \n",
    "\n",
    "$$ S = \\frac{1}{T} \\sum_{i=1}^{T} \\left(\\mathbb{I}[u_i = v_i]\\left( 1 + \\frac{1}{2}s(f_i, \\phi_i)\\right) - 1\\right)$$\n",
    "\n",
    "$$ s(f, \\phi) = 1 + \\frac{2 |f \\cap \\phi| - |f \\cup \\phi|}{\\max(|f|, |\\phi|)} $$\n",
    "\n",
    "Метрика учитывает, что предсказать правильную утилиту важнее чем правильный флаг. При этом порядок флагов не важен (однако, чтобы корректно "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "PATH_TO_CLAI_UTILS = './utils'\n",
    "sys.path.append(PATH_TO_CLAI_UTILS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting bashlex grammar using file: /home/bicdan22/hw/mmot/hw4/./utils/bashlint/grammar/grammar100.txt\n",
      "Bashlint grammar set up (148 utilities)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from bashlint.data_tools import bash_parser, pretty_print, cmd2template\n",
    "from metric.metric_utils import compute_metric\n",
    "from functools import partial\n",
    "\n",
    "from collections import Counter\n",
    "import sentencepiece as spm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем данные. В столбце `invocation` находится текстовый запрос, в столбце `cmd` находится релевантная команда."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>copy loadable kernel module \"mymodule.ko\" to t...</td>\n",
       "      <td>sudo cp mymodule.ko /lib/modules/$(uname -r)/k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>display all lines containing \"ip_mroute\" in th...</td>\n",
       "      <td>cat /boot/config-`uname -r` | grep IP_MROUTE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>display current running kernel's compile-time ...</td>\n",
       "      <td>cat /boot/config-`uname -r`</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>find all loadable modules for current kernel, ...</td>\n",
       "      <td>find /lib/modules/`uname -r` -regex .*perf.*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>look for any instance of \"highmem\" in the curr...</td>\n",
       "      <td>grep “HIGHMEM” /boot/config-`uname -r`</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          invocation  \\\n",
       "0  copy loadable kernel module \"mymodule.ko\" to t...   \n",
       "1  display all lines containing \"ip_mroute\" in th...   \n",
       "2  display current running kernel's compile-time ...   \n",
       "3  find all loadable modules for current kernel, ...   \n",
       "4  look for any instance of \"highmem\" in the curr...   \n",
       "\n",
       "                                                 cmd  \n",
       "0  sudo cp mymodule.ko /lib/modules/$(uname -r)/k...  \n",
       "1       cat /boot/config-`uname -r` | grep IP_MROUTE  \n",
       "2                        cat /boot/config-`uname -r`  \n",
       "3       find /lib/modules/`uname -r` -regex .*perf.*  \n",
       "4             grep “HIGHMEM” /boot/config-`uname -r`  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('data/train.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В тестовых данных столбец `origin` отвечает за источник данных, значения `handrafted` соответствуют парам, составленными людьми, а `mined` парам, собранным автоматически."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "      <th>origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>create ssh connection to specified ip from spe...</td>\n",
       "      <td>ssh user123@176.0.13.154</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>search for commands containing string \"zeppeli...</td>\n",
       "      <td>history | grep zeppelin</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>search for location of specified file or appli...</td>\n",
       "      <td>whereis python3</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>grant all rights to root folder</td>\n",
       "      <td>sudo chmod 777 -R /</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>search in running processes for specified name</td>\n",
       "      <td>ps -aux | grep zepp</td>\n",
       "      <td>handcrafted</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          invocation  \\\n",
       "0  create ssh connection to specified ip from spe...   \n",
       "1  search for commands containing string \"zeppeli...   \n",
       "2  search for location of specified file or appli...   \n",
       "3                    grant all rights to root folder   \n",
       "4     search in running processes for specified name   \n",
       "\n",
       "                        cmd       origin  \n",
       "0  ssh user123@176.0.13.154  handcrafted  \n",
       "1   history | grep zeppelin  handcrafted  \n",
       "2           whereis python3  handcrafted  \n",
       "3       sudo chmod 777 -R /  handcrafted  \n",
       "4       ps -aux | grep zepp  handcrafted  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('data/test.csv')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Проведите предобработку текста. Рекомендуется:\n",
    "* перевести всё в нижний регистр\n",
    "* удалить стоп-слова (специфичные для выборки)\n",
    "* провести стемминг токенов\n",
    "* удалить все символы кроме латинских букв"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bicdan22/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z]', ' ', text)\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "    stemmed_words = [porter.stem(word) for word in filtered_words]\n",
    "    \n",
    "    clean_text = ' '.join(stemmed_words)\n",
    "    \n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['text_cleaned'] = train_data['invocation'].apply(clean_text)\n",
    "test_data['text_cleaned'] = test_data['invocation'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для обработки кода воспользуемся функцией `cmd2template`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['cmd_cleaned'] = train_data['cmd'].apply(partial(cmd2template, loose_constraints=True))\n",
    "test_data['cmd_cleaned'] = test_data['cmd'].apply(partial(cmd2template, loose_constraints=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучение и валидацию. Т.к. данных очень мало, то для валидационной выборки выделим только 100 примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = train_data.iloc[-100:]\n",
    "train_data = train_data.iloc[:-100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>invocation</th>\n",
       "      <th>cmd</th>\n",
       "      <th>text_cleaned</th>\n",
       "      <th>cmd_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9843</th>\n",
       "      <td>searches through the root filesystem (\"/\") for...</td>\n",
       "      <td>find / -name Chapter1 -type f -print</td>\n",
       "      <td>search root filesystem file name chapter print...</td>\n",
       "      <td>find Path -name Regex -type f -print</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9844</th>\n",
       "      <td>searches through the root filesystem (\"/\") for...</td>\n",
       "      <td>find / -name Chapter1 -type f</td>\n",
       "      <td>search root filesystem file name chapter</td>\n",
       "      <td>find Path -name Regex -type f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9845</th>\n",
       "      <td>searches through the root filesystem (\"/\") for...</td>\n",
       "      <td>find / -name Chapter1 -type f -print</td>\n",
       "      <td>search root filesystem file name chapter</td>\n",
       "      <td>find Path -name Regex -type f -print</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9846</th>\n",
       "      <td>searching for all files with the extension mp3</td>\n",
       "      <td>find / -name *.mp3</td>\n",
       "      <td>search file extens mp</td>\n",
       "      <td>find Path -name Regex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9847</th>\n",
       "      <td>set myvariable to the value of variable_name</td>\n",
       "      <td>myVariable=$(env  | grep VARIABLE_NAME | grep ...</td>\n",
       "      <td>set myvari valu variabl name</td>\n",
       "      <td>env | grep Regex | grep -o -e Regex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9938</th>\n",
       "      <td>using exec in find command to dispaly the sear...</td>\n",
       "      <td>find . ... -exec cat {} \\; -exec echo \\;</td>\n",
       "      <td>use exec find command dispali search file</td>\n",
       "      <td>find Path Path -exec cat {} \\; -exec echo \\;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9939</th>\n",
       "      <td>verbosely create intermediate directoriy tmp a...</td>\n",
       "      <td>mkdir -pv /tmp/boostinst</td>\n",
       "      <td>verbos creat intermedi directoriy tmp requir d...</td>\n",
       "      <td>mkdir -p -v Directory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9940</th>\n",
       "      <td>view the manual page of find</td>\n",
       "      <td>man find</td>\n",
       "      <td>view manual page find</td>\n",
       "      <td>man Regex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9941</th>\n",
       "      <td>wait 2 seconds and then print \"hello\"</td>\n",
       "      <td>echo \"hello `sleep 2 &amp;`\"</td>\n",
       "      <td>wait second print hello</td>\n",
       "      <td>echo $( sleep Timespan )</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9942</th>\n",
       "      <td>when using vi-insert keymap bind command \"\\c-v...</td>\n",
       "      <td>bind -m vi-insert '\"{\" \"\\C-v{}\\ei\"'</td>\n",
       "      <td>use vi insert keymap bind command c v ei key</td>\n",
       "      <td>bind -m Regex Regex</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             invocation  \\\n",
       "9843  searches through the root filesystem (\"/\") for...   \n",
       "9844  searches through the root filesystem (\"/\") for...   \n",
       "9845  searches through the root filesystem (\"/\") for...   \n",
       "9846     searching for all files with the extension mp3   \n",
       "9847       set myvariable to the value of variable_name   \n",
       "...                                                 ...   \n",
       "9938  using exec in find command to dispaly the sear...   \n",
       "9939  verbosely create intermediate directoriy tmp a...   \n",
       "9940                       view the manual page of find   \n",
       "9941              wait 2 seconds and then print \"hello\"   \n",
       "9942  when using vi-insert keymap bind command \"\\c-v...   \n",
       "\n",
       "                                                    cmd  \\\n",
       "9843               find / -name Chapter1 -type f -print   \n",
       "9844                      find / -name Chapter1 -type f   \n",
       "9845               find / -name Chapter1 -type f -print   \n",
       "9846                                 find / -name *.mp3   \n",
       "9847  myVariable=$(env  | grep VARIABLE_NAME | grep ...   \n",
       "...                                                 ...   \n",
       "9938           find . ... -exec cat {} \\; -exec echo \\;   \n",
       "9939                           mkdir -pv /tmp/boostinst   \n",
       "9940                                           man find   \n",
       "9941                           echo \"hello `sleep 2 &`\"   \n",
       "9942                bind -m vi-insert '\"{\" \"\\C-v{}\\ei\"'   \n",
       "\n",
       "                                           text_cleaned  \\\n",
       "9843  search root filesystem file name chapter print...   \n",
       "9844           search root filesystem file name chapter   \n",
       "9845           search root filesystem file name chapter   \n",
       "9846                              search file extens mp   \n",
       "9847                       set myvari valu variabl name   \n",
       "...                                                 ...   \n",
       "9938          use exec find command dispali search file   \n",
       "9939  verbos creat intermedi directoriy tmp requir d...   \n",
       "9940                              view manual page find   \n",
       "9941                            wait second print hello   \n",
       "9942       use vi insert keymap bind command c v ei key   \n",
       "\n",
       "                                       cmd_cleaned  \n",
       "9843          find Path -name Regex -type f -print  \n",
       "9844                 find Path -name Regex -type f  \n",
       "9845          find Path -name Regex -type f -print  \n",
       "9846                         find Path -name Regex  \n",
       "9847           env | grep Regex | grep -o -e Regex  \n",
       "...                                            ...  \n",
       "9938  find Path Path -exec cat {} \\; -exec echo \\;  \n",
       "9939                         mkdir -p -v Directory  \n",
       "9940                                     man Regex  \n",
       "9941                      echo $( sleep Timespan )  \n",
       "9942                           bind -m Regex Regex  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Стандартный формат входных данных для трансформеров — BPE токены. Воспользуйтесь библиотекой sentencepiece для обучения токенайзеров для текста и кода. Используйте небольшое число токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текстовый корпус до дедупликации: 10664\n",
      "Текстовый корпус после дедупликации: 9720\n"
     ]
    }
   ],
   "source": [
    "text_corpus = [row for row in train_data.text_cleaned]\n",
    "text_corpus += [row for row in valid_data.text_cleaned]\n",
    "text_corpus += [row for row in test_data.text_cleaned]\n",
    "print(f'Текстовый корпус до дедупликации: {len(text_corpus)}')\n",
    "\n",
    "# Дедупликация запросов\n",
    "text_corpus = [row for row in set(text_corpus)]\n",
    "print(f'Текстовый корпус после дедупликации: {len(text_corpus)}')\n",
    "\n",
    "with open('./tokenizer/text_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in text_corpus:\n",
    "        f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Командный корпус до дедупликации: 10664\n",
      "Командный корпус после дедупликации: 5554\n"
     ]
    }
   ],
   "source": [
    "cmd_corpus = [row for row in train_data.cmd_cleaned]\n",
    "cmd_corpus += [row for row in valid_data.cmd_cleaned]\n",
    "cmd_corpus += [row for row in test_data.cmd_cleaned]\n",
    "print(f'Командный корпус до дедупликации: {len(cmd_corpus)}')\n",
    "\n",
    "# Дедупликация запросов\n",
    "cmd_corpus = [row for row in set(cmd_corpus)]\n",
    "print(f'Командный корпус после дедупликации: {len(cmd_corpus)}')\n",
    "\n",
    "with open('./tokenizer/cmd_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in cmd_corpus:\n",
    "        f.write(text + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(input_file, model_prefix, vocab_size):\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=input_file,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./tokenizer/text_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: ./tokenizer/text_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 2500\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: ./tokenizer/text_corpus.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 9720 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=501951\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=27\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=1\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 9720 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=293235\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 7259 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 9720\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 3338\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 3338 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=3160 obj=8.25695 num_tokens=5943 num_tokens/piece=1.8807\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=2703 obj=6.35492 num_tokens=5892 num_tokens/piece=2.1798\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ./tokenizer/text_tokenizer.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ./tokenizer/text_tokenizer.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: ./tokenizer/cmd_corpus.txt\n",
      "  input_format: \n",
      "  model_prefix: ./tokenizer/cmd_tokenizer\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 600\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(351) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(183) LOG(INFO) Loading corpus: ./tokenizer/cmd_corpus.txt\n",
      "trainer_interface.cc(407) LOG(INFO) Loaded all 5553 sentences\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(423) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(428) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(537) LOG(INFO) all chars count=242791\n",
      "trainer_interface.cc(548) LOG(INFO) Done: 99.9514% characters are covered.\n",
      "trainer_interface.cc(558) LOG(INFO) Alphabet size=72\n",
      "trainer_interface.cc(559) LOG(INFO) Final character coverage=0.999514\n",
      "trainer_interface.cc(591) LOG(INFO) Done! preprocessed 5553 sentences.\n",
      "unigram_model_trainer.cc(222) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(226) LOG(INFO) Extracting frequent sub strings... node_num=157955\n",
      "unigram_model_trainer.cc(274) LOG(INFO) Initialized 1286 seed sentencepieces\n",
      "trainer_interface.cc(597) LOG(INFO) Tokenizing input sentences with whitespace: 5553\n",
      "trainer_interface.cc(608) LOG(INFO) Done! 792\n",
      "unigram_model_trainer.cc(564) LOG(INFO) Using 792 sentences for EM training\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=0 size=800 obj=7.91332 num_tokens=2879 num_tokens/piece=3.59875\n",
      "unigram_model_trainer.cc(580) LOG(INFO) EM sub_iter=1 size=640 obj=6.2721 num_tokens=2920 num_tokens/piece=4.5625\n",
      "trainer_interface.cc(686) LOG(INFO) Saving model: ./tokenizer/cmd_tokenizer.model\n",
      "trainer_interface.cc(698) LOG(INFO) Saving vocabs: ./tokenizer/cmd_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "train_tokenizer(\"./tokenizer/text_corpus.txt\", \"./tokenizer/text_tokenizer\", vocab_size=2500)\n",
    "text_tokenizer = spm.SentencePieceProcessor(model_file=\"./tokenizer/text_tokenizer.model\")\n",
    "\n",
    "train_tokenizer(\"./tokenizer/cmd_corpus.txt\", \"./tokenizer/cmd_tokenizer\", vocab_size=600)\n",
    "cmd_tokenizer = spm.SentencePieceProcessor(model_file=\"./tokenizer/cmd_tokenizer.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Задайте датасеты и лоадеры для ваших данных. Каждая последовательность должна начинаться с BOS токена и заканчиваться EOS токеном. Рекомендуется ограничить длину входных и выходных последовательностей!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "\n",
    "\n",
    "MAX_TEXT_LENGTH = 256\n",
    "MAX_CODE_LENGTH = 40\n",
    "\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToBashDataset(Dataset):\n",
    "    def __init__(self, data, text_tokenizer, cmd_tokenizer,\n",
    "                 max_text_length=MAX_TEXT_LENGTH,\n",
    "                 max_code_length=MAX_CODE_LENGTH):\n",
    "        self.data = data\n",
    "        self.text_tokenizer = text_tokenizer\n",
    "        self.cmd_tokenizer = cmd_tokenizer\n",
    "        self.max_text_length = max_text_length\n",
    "        self.max_code_length = max_code_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data.iloc[idx]['text_cleaned']\n",
    "        cmd = self.data.iloc[idx]['cmd_cleaned']\n",
    "\n",
    "        # Токенизация и добавление BOS и EOS токенов\n",
    "        text_tokens = [BOS_ID] + self.text_tokenizer.encode(text)[:self.max_text_length - 2] + [EOS_ID]\n",
    "        cmd_tokens = [BOS_ID] + self.cmd_tokenizer.encode(cmd)[:self.max_code_length - 2] + [EOS_ID]\n",
    "\n",
    "        # Дополнение нулями до максимальной длины\n",
    "        text_padding = [PAD_ID] * (self.max_text_length - len(text_tokens))\n",
    "        cmd_padding = [PAD_ID] * (self.max_code_length - len(cmd_tokens))\n",
    "\n",
    "        text_tokens += text_padding\n",
    "        cmd_tokens += cmd_padding\n",
    "\n",
    "        return torch.Tensor(text_tokens).long(), torch.Tensor(cmd_tokens).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TextToBashDataset(train_data, text_tokenizer, cmd_tokenizer)\n",
    "valid_ds = TextToBashDataset(valid_data, text_tokenizer, cmd_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaders = {\n",
    "    'train': DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True),\n",
    "    'valid': DataLoader(valid_ds, batch_size=BATCH_SIZE),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение бейзлайна (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertModel, EncoderDecoderConfig, EncoderDecoderModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание.** Реализуйте модель encoder-decoder ниже. В качестве моделей энкодера и декодера рекомендуется использовать BertModel из библиотеки transformers, заданную через BertConfig. В случае декодера необходимо выставить параметры is_decoder=True и add_cross_attention=True. В качестве модели, <<сцепляющей>> энкодер и декодер, в одну архитектуру рекомендуется использовать EncoderDecoderModel.\n",
    "\n",
    "**Обратите внимание!** EncoderDecoderModel поддерживает использование кэшированных результатов при последовательной генерации. Это пригодится при реализации beam-search ниже.\n",
    "\n",
    "Для того, чтобы удобнее задавать модели, рекомендуется реализовать задание модели через конфиг. Ниже представлены базовые параметры, при которых модель должна работать быстро и с приемлемым качеством."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model_config = {\n",
    "    'vocab': text_tokenizer.vocab_size(),\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 2,\n",
    "    'num_attention_heads': 8,\n",
    "    'intermediate_size': 256 * 4,\n",
    "    'hidden_dropout_prob': 0.1,\n",
    "    'pad_id': PAD_ID,\n",
    "}\n",
    "\n",
    "cmd_model_config = {\n",
    "    'vocab': cmd_tokenizer.vocab_size(),\n",
    "    'hidden_size': 256,\n",
    "    'num_layers': 2,\n",
    "    'num_attention_heads': 8,\n",
    "    'intermediate_size': 256 * 4,\n",
    "    'hidden_dropout_prob': 0.1,\n",
    "    'pad_id': PAD_ID,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextToBashModel(nn.Module):\n",
    "    def __init__(self, text_model_config, cmd_model_config):\n",
    "        super(TextToBashModel, self).__init__()\n",
    "\n",
    "        # Конфигурация энкодера для текста\n",
    "        text_encoder_config = BertConfig(\n",
    "            vocab_size=text_model_config['vocab'],\n",
    "            hidden_size=text_model_config['hidden_size'],\n",
    "            num_hidden_layers=text_model_config['num_layers'],\n",
    "            num_attention_heads=text_model_config['num_attention_heads'],\n",
    "            intermediate_size=text_model_config['intermediate_size'],\n",
    "            hidden_dropout_prob=text_model_config['hidden_dropout_prob'],\n",
    "            pad_token_id=text_model_config['pad_id'],\n",
    "        )\n",
    "        \n",
    "        # Создание энкодера для текста\n",
    "        self.text_encoder = BertModel(text_encoder_config)\n",
    "        \n",
    "        # Конфигурация декодера для команд\n",
    "        cmd_decoder_config = BertConfig(\n",
    "            vocab_size=cmd_model_config['vocab'],\n",
    "            hidden_size=cmd_model_config['hidden_size'],\n",
    "            num_hidden_layers=cmd_model_config['num_layers'],\n",
    "            num_attention_heads=cmd_model_config['num_attention_heads'],\n",
    "            intermediate_size=cmd_model_config['intermediate_size'],\n",
    "            hidden_dropout_prob=cmd_model_config['hidden_dropout_prob'],\n",
    "            pad_token_id=cmd_model_config['pad_id'],\n",
    "            is_decoder=True,\n",
    "            add_cross_attention=True,\n",
    "        )\n",
    "        \n",
    "        # Создание декодера для команд\n",
    "        self.cmd_decoder = BertModel(cmd_decoder_config)\n",
    "        \n",
    "        # Конфигурация для сцепления энкодера и декодера в одну модель\n",
    "        encoder_decoder_config = EncoderDecoderConfig.from_encoder_decoder_configs(text_encoder_config, cmd_decoder_config)\n",
    "        \n",
    "        # Создание модели, объединяющей энкодер и декодер\n",
    "        self.model = EncoderDecoderModel(encoder_decoder_config)\n",
    "        \n",
    "    def forward(self, input_ids, decoder_input_ids):\n",
    "        # Forward pass через энкодер для текста\n",
    "        text_encoder_output = self.text_encoder(input_ids=input_ids)\n",
    "        \n",
    "        # Forward pass через декодер для команд\n",
    "        cmd_decoder_output = self.cmd_decoder(input_ids=decoder_input_ids, encoder_hidden_states=text_encoder_output.last_hidden_state)\n",
    "        \n",
    "        # Forward pass через общую модель\n",
    "        model_output = self.model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "        \n",
    "        return model_output.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Обучите вашу модель ниже.\n",
    "\n",
    "Рекомендуется:\n",
    "* в качестве лосса использовать стандартную кросс-энтропию, не забывайте игнорировать PAD токены\n",
    "* использовать Adam для оптимизации\n",
    "* не использовать scheduler для бейзлайна (модель легко переобучается с ним)\n",
    "* использовать early stopping по валидационному лоссу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/82 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "100%|██████████| 82/82 [02:47<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.8263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation Loss: 0.1671\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [02:39<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Train Loss: 0.0398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Validation Loss: 0.0683\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [02:39<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Train Loss: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Validation Loss: 0.0424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [02:39<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Train Loss: 0.0079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Validation Loss: 0.0351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [02:40<00:00,  1.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Train Loss: 0.0045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Validation Loss: 0.0321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [02:38<00:00,  1.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Train Loss: 0.0029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Validation Loss: 0.0308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [02:43<00:00,  1.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Train Loss: 0.0021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Validation Loss: 0.0301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [02:42<00:00,  1.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Train Loss: 0.0016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Validation Loss: 0.0297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [02:45<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Train Loss: 0.0013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Validation Loss: 0.0295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 82/82 [02:40<00:00,  1.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Train Loss: 0.0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Validation Loss: 0.0292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Разделение данных на обучающую и валидационную выборки\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.1, random_state=42)\n",
    "\n",
    "# Создание экземпляра модели\n",
    "model = TextToBashModel(text_model_config, cmd_model_config)\n",
    "\n",
    "# Определение устройства (CPU или GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Критерий (лосс) - кросс-энтропия\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID)\n",
    "\n",
    "# Оптимизатор - Adam\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "# Количество эпох\n",
    "num_epochs = 10\n",
    "\n",
    "# Early stopping\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "counter = 0\n",
    "\n",
    "# Цикл обучения\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Итерация по обучающему датасету\n",
    "    for batch in tqdm(loaders['train']):\n",
    "        input_ids = batch[0].to(device)\n",
    "        decoder_input_ids = batch[1].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        logits = model(input_ids, decoder_input_ids)\n",
    "        \n",
    "        # Подсчет лосса\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), decoder_input_ids.view(-1))\n",
    "        \n",
    "        # Backward pass и обновление весов\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    # Вычисление среднего лосса для текущей эпохи\n",
    "    average_loss = total_loss / len(loaders['train'])\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {average_loss:.4f}\")\n",
    "    \n",
    "    # Валидация модели\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Итерация по валидационному датасету\n",
    "        for batch in tqdm(loaders['valid']):\n",
    "            input_ids = batch[0].to(device)\n",
    "            decoder_input_ids = batch[1].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(input_ids, decoder_input_ids)\n",
    "            \n",
    "            # Подсчет лосса\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), decoder_input_ids.view(-1))\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "    \n",
    "    # Вычисление среднего валидационного лосса\n",
    "    average_val_loss = val_loss / len(loaders['valid'])\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Validation Loss: {average_val_loss:.4f}\")\n",
    "    \n",
    "    # Проверка Early stopping\n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Генерация команд (2 балла)\n",
    "\n",
    "**Задание**. Реализуйте алгоритм beam-search в классе BeamSearchGenerator ниже. Ваша реализация должна поддерживать задание температуры софтмакса. Выходы модели, полученные на предыдущих итерациях, необходимо кэшировать для повышения скорости алгоритма. Вместо подсчёта произведения любых вероятностей необходимо считать сумму их логарифмов.\n",
    "\n",
    "Алгоритм должен возвращать список пар из получившихся выходных последовательностей и логарифмов их вероятностей. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BeamSearchGenerator:\n",
    "    def __init__(\n",
    "            self, pad_id, eos_id, bos_id,\n",
    "            max_length=20, beam_width=5, temperature=1,\n",
    "            device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        pad_id : int\n",
    "        eos_id : int\n",
    "        bos_id : int\n",
    "        max_length : int\n",
    "            Maximum length of output sequence\n",
    "        beam_width : int\n",
    "            Width of the beam\n",
    "        temperature : float\n",
    "            Softmax temperature\n",
    "        device : torch.device\n",
    "            Your model device\n",
    "        \"\"\"\n",
    "        self.pad_id = pad_id\n",
    "        self.eos_id = eos_id\n",
    "        self.bos_id = bos_id\n",
    "        \n",
    "        self.max_length = max_length\n",
    "        self.beam_width = beam_width\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        self.device = device\n",
    "        \n",
    "    def get_result(self, model, input_text_tokens):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : TextToBashModel\n",
    "        input_text_tokens : torch.tensor\n",
    "            One object input tensor\n",
    "        \"\"\"\n",
    "        # Запуск энкодера на входном тексте\n",
    "        encoder_output = model.text_encoder(input_ids=input_text_tokens.unsqueeze(0).to(self.device))\n",
    "        \n",
    "        # Начальное состояние beam search\n",
    "        beam_search_state = [{\n",
    "            'sequence': [self.bos_id],\n",
    "            'log_prob': 0.0,\n",
    "            'length': 1,\n",
    "            'last_hidden_state': encoder_output.last_hidden_state,\n",
    "        }]\n",
    "        \n",
    "        # Итерации по длине последовательности\n",
    "        for length in range(1, self.max_length + 1):\n",
    "            # Список для хранения нового состояния beam search\n",
    "            new_beam_search_state = []\n",
    "            \n",
    "            # Итерации по текущему состоянию beam search\n",
    "            for state in beam_search_state:\n",
    "                # Если последовательность достигла максимальной длины, добавляем в результат\n",
    "                if state['sequence'][-1] == self.eos_id or state['length'] == self.max_length:\n",
    "                    new_beam_search_state.append(state)\n",
    "                else:\n",
    "                    # Подготавливаем вход для декодера\n",
    "                    decoder_input_ids = torch.tensor([state['sequence']], device=self.device)\n",
    "                    \n",
    "                    # Forward pass декодера\n",
    "                    logits = model.cmd_decoder(input_ids=decoder_input_ids, encoder_hidden_states=state['last_hidden_state'])[0]\n",
    "                    logits = logits[:, -1, :] / self.temperature\n",
    "                    \n",
    "                    # Применение softmax для получения вероятностей\n",
    "                    probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "                    \n",
    "                    # Получение top-k индексов\n",
    "                    top_k_probs, top_k_indices = torch.topk(probabilities, self.beam_width, dim=-1)\n",
    "                    \n",
    "                    # Расчет нового состояния beam search для каждого top-k индекса\n",
    "                    for k in range(self.beam_width):\n",
    "                        new_sequence = state['sequence'] + [top_k_indices[0, k].item()]\n",
    "                        new_log_prob = state['log_prob'] + top_k_probs[0, k].log().item()\n",
    "                        new_length = state['length'] + 1\n",
    "                        \n",
    "                        # Кэширование результатов предыдущей итерации для ускорения\n",
    "                        new_last_hidden_state = state['last_hidden_state']\n",
    "                        if k > 0:\n",
    "                            new_last_hidden_state = torch.cat([new_last_hidden_state, state['last_hidden_state']], dim=0)\n",
    "                        \n",
    "                        new_beam_search_state.append({\n",
    "                            'sequence': new_sequence,\n",
    "                            'log_prob': new_log_prob,\n",
    "                            'length': new_length,\n",
    "                            'last_hidden_state': new_last_hidden_state,\n",
    "                        })\n",
    "            \n",
    "            # Сортировка нового состояния beam search по логарифму вероятности\n",
    "            beam_search_state = sorted(new_beam_search_state, key=lambda x: x['log_prob'], reverse=True)[:self.beam_width]\n",
    "        \n",
    "        # Отбор лучших результатов\n",
    "        best_results = sorted(beam_search_state, key=lambda x: x['log_prob'], reverse=True)\n",
    "        \n",
    "        # Возвращение списка пар результатов\n",
    "        return [(result['sequence'], result['log_prob']) for result in best_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Протестируйте на нескольких примерах работу вашего алгоритма. Если всё реализовано правильно, то как минимум на трёх примерах из 5 всё должно работать правильно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_search_engine = BeamSearchGenerator(\n",
    "    pad_id=PAD_ID, eos_id=EOS_ID, bos_id=BOS_ID,\n",
    "    max_length=MAX_CODE_LENGTH, beam_width=5,\n",
    "    temperature=1, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "text: searches through the root filesystem (\"/\") for the file named chapter1, and prints the location\n",
      "true: find / -name Chapter1 -type f -print\n",
      "true cleaned: find Path -name Regex -type f -print\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"scp Permission -120.37553215026855\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"scpS -120.6160933971405\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"okor -120.67719459533691\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ok\"scp Permission -120.80940866470337\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@<\"scp Permission -120.84984683990479\n",
      "-1.0\n",
      "\n",
      "text: searches through the root filesystem (\"/\") for the file named chapter1.\n",
      "true: find / -name Chapter1 -type f\n",
      "true cleaned: find Path -name Regex -type f\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"scp Permission -120.37127339839935\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"scpS -120.61192047595978\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"okor -120.67319452762604\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ok\"scp Permission -120.80628097057343\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@<\"scp Permission -120.84635031223297\n",
      "-1.0\n",
      "\n",
      "text: searches through the root filesystem (\"/\") for the file named chapter1.\n",
      "true: find / -name Chapter1 -type f -print\n",
      "true cleaned: find Path -name Regex -type f -print\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"scp Permission -120.37127339839935\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"scpS -120.61192047595978\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"okor -120.67319452762604\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ok\"scp Permission -120.80628097057343\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@<\"scp Permission -120.84635031223297\n",
      "-1.0\n",
      "\n",
      "text: searching for all files with the extension mp3\n",
      "true: find / -name *.mp3\n",
      "true cleaned: find Path -name Regex\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"scp Permission -120.36968064308167\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"scpS -120.61112093925476\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"okor -120.6735327243805\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ok\"scp Permission -120.80448913574219\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@<\"scp Permission -120.84363150596619\n",
      "-1.0\n",
      "\n",
      "text: set myvariable to the value of variable_name\n",
      "true: myVariable=$(env  | grep VARIABLE_NAME | grep -oe '[^=]*$');\n",
      "true cleaned: env | grep Regex | grep -o -e Regex\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"scp Permission -120.36982989311218\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"scpS -120.61026215553284\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ -2\"okor -120.67088460922241\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@ok\"scp Permission -120.80463171005249\n",
      "prune -2prune bas bind -2amensize ifconfig chgrp ' \"%prunename DateTime -- true read bindand \\o Path set ifconfig-%scp3@ -2name3@<\"scp Permission -120.8446888923645\n",
      "-1.0\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(5):\n",
    "        print()\n",
    "        print('text:', valid_data.invocation.iloc[i])\n",
    "        print('true:', valid_data.cmd.iloc[i])\n",
    "        print('true cleaned:', valid_data.cmd_cleaned.iloc[i])\n",
    "\n",
    "        src = valid_ds[i][0]\n",
    "        pred = beam_search_engine.get_result(model, src)\n",
    "        \n",
    "        scores = []\n",
    "        for x, proba in pred:\n",
    "            pred_cmd = cmd_tokenizer.decode(list(map(int, x)))\n",
    "            score = compute_metric(pred_cmd, 1, valid_data.cmd.iloc[i])\n",
    "            scores.append(score)\n",
    "            print(pred_cmd, proba)\n",
    "        print(max(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание**. Дополните функцию для подсчёта качества. Посчитайте качество вашей модели на валидационном и тестовых датасетов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_all_scores(model, df, beam_engine):\n",
    "    all_scores = []\n",
    "\n",
    "    for i, (text, target_cmd) in enumerate(zip(df.text_cleaned.values, df.cmd.values)):\n",
    "        input_tokens = [text_tokenizer.bos_id()] + text_tokenizer.encode(text)\n",
    "        predictions = beam_engine.get_result(model, input_tokens)\n",
    "        \n",
    "        # get only 5 top results\n",
    "        predictions = predictions[:5]\n",
    "        object_scores = []\n",
    "        for output_tokens, proba in predictions:\n",
    "            output_cmd = cmd_tokenizer.decode(list(map(int, output_tokens)))\n",
    "            score = compute_metric(output_cmd, 1, target_cmd)\n",
    "            object_scores.append(score)\n",
    "        \n",
    "        all_scores.append(max(object_scores))\n",
    "    return all_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ваша цель при помощи подбора параметров модели и генерации получить средний скор на валидации >= 0.2, скор `handcrafted` части теста >= 0.1. На `mined` части датасета скор может быть низкий, т.к. некоторых команд из датасета нет в обучении.\n",
    "\n",
    "**Обратите внимание.** Так как датасет для обучения не очень большой, а данные достаточно нестабильные, подбор параметров может очень сильно влиять на модель. Некоторые полезные советы:\n",
    "* Отслеживайте качество модели после каждой эпохи, не забывайте про early stopping\n",
    "* Вы можете сразу приступить к следующей части. Побитие скора в этой части задания при помощи трюков из бонусной части считается валидным."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшение модели (4 балла)\n",
    "\n",
    "Вы реализовали бейзлайн, пришло время улучшить качество модели. Т.к. это последнее задание, мы не будем предлагать конкретные шаги, а только дадим несколько советов.\n",
    "\n",
    "1. Большой источник информации о работе командной строке — её документация, man. Один из способов улучшения модели - использование мана для генерации новых примеров. Структурированный ман можно найти по ссылке https://github.com/IBM/clai/blob/nlc2cmd/docs/manpage-data.md.\n",
    "2. Ещё один способ улучшить модель, разделить предсказание утилит и флагов. Т.к. задача предсказания утилит более важная, вы можете натренировать модель, которая предсказывает последовательность утилит, а затем к каждой утилите генерировать флаги.\n",
    "3. Можно аугментировать данные, чтобы увеличить выборку.\n",
    "4. Можно в качество входа подавать не только текстовый запрос, но и описание из мана. Т.к. всё описание достаточно большое, нужно сделать дополнительную модель, которая будет выбирать команды, для которых нужно вытащить описание.\n",
    "5. Найти дополнительные данные, улучшающие обучение\n",
    "6. Как всегда можно просто сделать больше слоёв, увеличить размер скрытого слоя и т.д."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "От вас ожидается скор на валидации >= 0.25, `mined` >= 0, `handrafted` >= 0.15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOUR CODE HERE ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Бонусные баллы (до 3 баллов)\n",
    "\n",
    "При существенном улучшении качества будут назначаться бонусные баллы. На тестовых датасетах реально выбить качество >= 0.3 на каждом, но усилий потребуется немало..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
